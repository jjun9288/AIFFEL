{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bearing-excuse",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hybrid-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indirect-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "entire-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples : \n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = []\n",
    "\n",
    "#여러개의 txt파일을 모두 읽어서 raw_corpus에 담는다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples : \\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-penguin",
   "metadata": {},
   "source": [
    "### 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "banner-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence. <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\"\\1\", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+',\" \",sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\",\" \",sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "#test\n",
    "print(preprocess_sentence(\"This @_is ;;;;sample          sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "shared-georgia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> hook <end>',\n",
       " '<start> hook <end>',\n",
       " '<start> i ve been down so long, it look like up to me <end>',\n",
       " '<start> i ve been down so long, it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin fake love to me <end>',\n",
       " '<start> i got fake people showin fake love to me <end>',\n",
       " '<start> straight up to my face, straight up to my face <end>',\n",
       " '<start> straight up to my face, straight up to my face <end>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0 : continue\n",
    "    if sentence[-1] == ':' : continue\n",
    "    temp = preprocess_sentence(sentence)\n",
    "    if len(temp.split()) > 15 : continue\n",
    "    corpus.append(temp)\n",
    "        \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "#10개만 테스트\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "raised-bracelet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 12000, filters = '', oov_token = \"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "src_input = tensor[:,:-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "elect-restaurant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (256488, 14)\n",
      "Target Train: (256488, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-compound",
   "metadata": {},
   "source": [
    "### 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tight-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)  # 입력된 텐서에는 단어사전의 인덱스가 들어있는데, 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "embedding_size = 19\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words+1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "friendly-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1002/1002 [==============================] - 891s 864ms/step - loss: 3.6128 - val_loss: 2.8604\n",
      "Epoch 2/10\n",
      "1002/1002 [==============================] - 865s 863ms/step - loss: 2.7340 - val_loss: 2.5133\n",
      "Epoch 3/10\n",
      "1002/1002 [==============================] - 865s 863ms/step - loss: 2.3068 - val_loss: 2.2169\n",
      "Epoch 4/10\n",
      "1002/1002 [==============================] - 868s 867ms/step - loss: 1.9385 - val_loss: 1.9838\n",
      "Epoch 5/10\n",
      "1002/1002 [==============================] - 861s 859ms/step - loss: 1.6611 - val_loss: 1.8060\n",
      "Epoch 6/10\n",
      "1002/1002 [==============================] - 864s 863ms/step - loss: 1.4649 - val_loss: 1.6687\n",
      "Epoch 7/10\n",
      "1002/1002 [==============================] - 866s 864ms/step - loss: 1.3309 - val_loss: 1.5737\n",
      "Epoch 8/10\n",
      "1002/1002 [==============================] - 861s 859ms/step - loss: 1.2355 - val_loss: 1.5007\n",
      "Epoch 9/10\n",
      "1002/1002 [==============================] - 865s 863ms/step - loss: 1.1667 - val_loss: 1.4523\n",
      "Epoch 10/10\n",
      "1002/1002 [==============================] - 863s 861ms/step - loss: 1.1149 - val_loss: 1.4104\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "epochs = 10\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(enc_train, \n",
    "          dec_train, \n",
    "          epochs=epochs,\n",
    "          batch_size=256,\n",
    "          validation_data=(enc_val, dec_val),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "varying-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "partial-aurora",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-intervention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
